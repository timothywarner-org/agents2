# Token Tracking Implementation - Quick Summary

## What Was Added

Comprehensive **token usage tracking and cost estimation** for teaching responsible AI agent development.

## Files Modified

### Core Models (`src/agent_mvp/models.py`)
- **TokenUsage** - Per-call token stats (input, output, total, cost)
- **AgentTokens** - Token usage for specific agent
- **PipelineTokens** - Aggregated usage across all agents with efficiency metrics
- **RunMetadata.token_usage** - Added token tracking to run metadata
- **PipelineResult.metadata** - Now includes full token data

### Token Tracking Utility (`src/agent_mvp/util/token_tracking.py`) - NEW
- `extract_token_usage()` - Extracts tokens from LangChain response
- `calculate_cost()` - USD cost calculation with pricing table
- `aggregate_pipeline_tokens()` - Sums usage across agents
- `format_token_summary()` - Pretty console output
- **PRICING** - Per-model pricing (Claude, GPT-4, GPT-3.5, etc.)

### Pipeline Integration (`src/agent_mvp/pipeline/graph.py`)
- **PipelineState.token_usages** - Tracks tokens through state flow
- **pm_node()** - Captures PM agent tokens
- **dev_node()** - Captures Dev agent tokens
- **qa_node()** - Captures QA agent tokens
- **finalize_node()** - Aggregates and displays token summary

### Tests (`tests/test_token_tracking.py`) - NEW
- Cost calculation tests (Claude, GPT-4o, unknown models)
- Aggregation tests
- Model serialization tests
- 8 tests, all passing âœ…

## Output Examples

### Console (after each run)
```
============================================================
ðŸ“Š TOKEN USAGE SUMMARY
============================================================
    PM:   1,234 in +  2,456 out =   3,690 total ($0.009270)
   Dev:   2,345 in +  4,567 out =   6,912 total ($0.017355)
    QA:   1,890 in +  1,234 out =   3,124 total ($0.007935)
------------------------------------------------------------
TOTAL:   5,469 in +  8,257 out =  13,726 total
COST:    $0.034560
============================================================
Avg tokens/agent: 4,575
Max agent tokens: 6,912
Context usage:    3.46%
Input/Output:     0.662
============================================================
```

### JSON (`outgoing/*.json`)
```json
{
  "metadata": {
    "token_usage": {
      "agents": [
        {"agent_name": "PM", "usage": {...}},
        {"agent_name": "Dev", "usage": {...}},
        {"agent_name": "QA", "usage": {...}}
      ],
      "total_input_tokens": 5469,
      "total_output_tokens": 8257,
      "total_tokens": 13726,
      "estimated_total_cost_usd": 0.034560,
      "cost_breakdown": {"PM": 0.009270, ...},
      "efficiency_metrics": {
        "average_tokens_per_agent": 4575.33,
        "max_agent_tokens": 6912,
        "estimated_context_window_usage_percent": 3.46,
        "input_output_ratio": 0.662
      }
    }
  }
}
```

## Teaching Benefits

1. **Cost Awareness** - Students see real USD costs per agent
2. **Prompt Engineering** - Input/output ratio shows prompt efficiency
3. **Context Management** - Context % shows window utilization
4. **Production Skills** - Builds habits for monitoring in real systems
5. **Optimization** - Per-agent breakdown helps identify cost hot spots

## Documentation

- **[TOKEN_TRACKING.md](TOKEN_TRACKING.md)** - Comprehensive guide (300+ lines)
- **[README.md](README.md)** - Updated with token tracking feature
- Inline code documentation in all new functions

## Test Results

```bash
$ pytest tests/test_token_tracking.py -v
======================== 8 passed in 0.08s ========================
```

## Next Steps for Students

1. Run a pipeline and observe the token summary
2. Experiment with prompt length and see cost impact
3. Compare different models (Claude vs GPT) costs
4. Calculate monthly cost projections for production use
5. Optimize high-cost agents by refining prompts

---

**Status**: âœ… Complete and tested
**Impact**: High - Essential for cost-aware AI development
